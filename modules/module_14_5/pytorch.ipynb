{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7aa4c470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "183d2892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.tensor(4.)\n",
    "t1.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f010ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.tensor([1, 2, 3, 4], dtype=pt.float)\n",
    "t2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84e6bb50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.tensor([1., 2, 3, 4])\n",
    "t2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "619c4b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.,  6.],\n",
      "        [ 7.,  8.],\n",
      "        [ 9., 10.]])\n",
      "torch.Size([3, 2]) torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "t3 = torch.tensor([[5., 6], \n",
    "                   [7, 8], \n",
    "                   [9, 10]])\n",
    "print(t3)\n",
    "print(t3.shape, t3.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "469faeb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[11., 12., 13.],\n",
       "         [13., 14., 15.]],\n",
       "\n",
       "        [[15., 16., 17.],\n",
       "         [17., 18., 19.]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-dimensional array\n",
    "t4 = torch.tensor([\n",
    "    [[11, 12, 13], \n",
    "     [13, 14, 15]], \n",
    "    [[15, 16, 17], \n",
    "     [17, 18, 19.]]])\n",
    "t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7f9305b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "torch.Size([]) \n",
      "\n",
      "tensor([1., 2., 3., 4.])\n",
      "torch.Size([4]) \n",
      "\n",
      "tensor([[ 5.,  6.],\n",
      "        [ 7.,  8.],\n",
      "        [ 9., 10.]])\n",
      "torch.Size([3, 2]) \n",
      "\n",
      "tensor([[[11., 12., 13.],\n",
      "         [13., 14., 15.]],\n",
      "\n",
      "        [[15., 16., 17.],\n",
      "         [17., 18., 19.]]])\n",
      "torch.Size([2, 2, 3]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in (t1, t2, t3, t4):\n",
    "    print(t)\n",
    "    print(t.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b06d75e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 1 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/39/glh0vc5x3qq23jgtd82s68200000gn/T/ipykernel_79375/3725301859.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m torch.tensor([[5., 6, 11], \n\u001b[0m\u001b[1;32m      2\u001b[0m                    \u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                    [9, 10]])\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 3 at dim 1 (got 2)"
     ]
    }
   ],
   "source": [
    "torch.tensor([[5., 6, 11], \n",
    "                   [7, 8], \n",
    "                   [9, 10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8202ae2d",
   "metadata": {},
   "source": [
    "## Tensor operations and gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e4068e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.), tensor(4., requires_grad=True), tensor(5., requires_grad=True))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enable 'autograd' / 'automatic gradients'; automatically compute the deriviative of y WRT w and b.\n",
    "x = torch.tensor(3.)\n",
    "w = torch.tensor(4., requires_grad=True)\n",
    "b = torch.tensor(5., requires_grad=True)   \n",
    "x, w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2e1f91b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = w * x + b\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4c93d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute derivatives\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "effa1163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx: None\n",
      "dy/dw: tensor(3.)\n",
      "dy/db: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Display gradients\n",
    "print('dy/dx:', x.grad)\n",
    "print('dy/dw:', w.grad)\n",
    "print('dy/db:', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2804ebb5",
   "metadata": {},
   "source": [
    "# Tensor functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8404a482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42, 42],\n",
       "        [42, 42],\n",
       "        [42, 42]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t6 = torch.full((3, 2), 42)\n",
    "t6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "400619a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  6.],\n",
       "        [ 7.,  8.],\n",
       "        [ 9., 10.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8503d438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  6.],\n",
       "        [ 7.,  8.],\n",
       "        [ 9., 10.],\n",
       "        [42., 42.],\n",
       "        [42., 42.],\n",
       "        [42., 42.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t7 = torch.cat((t3, t6))\n",
    "t7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e22a305f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9589, -0.2794],\n",
       "        [ 0.6570,  0.9894],\n",
       "        [ 0.4121, -0.5440],\n",
       "        [-0.9165, -0.9165],\n",
       "        [-0.9165, -0.9165],\n",
       "        [-0.9165, -0.9165]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the sin of each element\n",
    "t8 = torch.sin(t7)\n",
    "t8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f220c54c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9589, -0.2794],\n",
       "         [ 0.6570,  0.9894]],\n",
       "\n",
       "        [[ 0.4121, -0.5440],\n",
       "         [-0.9165, -0.9165]],\n",
       "\n",
       "        [[-0.9165, -0.9165],\n",
       "         [-0.9165, -0.9165]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the shape of a tensor\n",
    "t9 = t8.reshape(3, 2, 2)\n",
    "t9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b3e445",
   "metadata": {},
   "source": [
    "# Interoperability with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c35198cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1, 2], \n",
    "              [3, 4.]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ba41b9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.from_numpy(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "70f31769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float64'), torch.float64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e940848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y.numpy()\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa0e18",
   "metadata": {},
   "source": [
    "The interoperability between PyTorch and Numpy is essential because most datasets you'll work with will likely be read and preprocessed as Numpy arrays.\n",
    "\n",
    "You might wonder why we need a library like PyTorch at all since Numpy already provides data structures and utilities for working with multi-dimensional numeric data. There are two main reasons:\n",
    "\n",
    "    Autograd: The ability to automatically compute gradients for tensor operations is essential for training deep learning models.\n",
    "    GPU support: While working with massive datasets and large models, PyTorch tensor operations can be performed efficiently using a Graphics Processing Unit (GPU). Computations that might typically take hours can be completed within minutes using GPUs.\n",
    "\n",
    "We'll leverage both these features of PyTorch extensively in this tutorial series.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c130d9",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5775a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n",
      "tensor([[-0.1963, -0.5610, -1.2942],\n",
      "        [-0.3675,  1.7887,  0.2730]], requires_grad=True)\n",
      "tensor([-1.1831, -1.8981], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')\n",
    "\n",
    "# Convert inputs and targets to tensors\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)\n",
    "\n",
    "# Weights and biases\n",
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "print(w)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "26d3993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x @ w.t() + b\n",
    "\n",
    "# @ is matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "76b62aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-108.7509,  102.8593],\n",
      "        [-151.2438,  139.5408],\n",
      "        [-168.5016,  221.6535],\n",
      "        [ -93.2124,   47.6356],\n",
      "        [-159.1795,  163.5728]], grad_fn=<AddBackward0>)\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "print(preds)\n",
    "# Compare with targets\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d98dc0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss\n",
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()\n",
    "\n",
    "# numel is number of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d440d28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25827.3594, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a966f90",
   "metadata": {},
   "source": [
    "## compute gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c3a5bc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1963, -0.5610, -1.2942],\n",
      "        [-0.3675,  1.7887,  0.2730]], requires_grad=True)\n",
      "tensor([[-17603.1367, -20024.8672, -12247.6816],\n",
      "        [  3555.8323,   4441.8120,   2507.0166]])\n"
     ]
    }
   ],
   "source": [
    "# Compute gradients\n",
    "loss.backward()\n",
    "print(w)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5b469067",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea41dcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25827.3594, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3c3fe75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]]) tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "print(w.grad, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d2c2d",
   "metadata": {},
   "source": [
    "## Train using grad descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9d4ec236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -77.2153,   96.2091],\n",
       "        [-109.7624,  130.7913],\n",
       "        [-119.2477,  211.1534],\n",
       "        [ -62.1127,   41.1707],\n",
       "        [-119.2340,  155.0998]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bb71c140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17636.4492, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse(preds, targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "abcdae0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-14345.0596, -16517.8047, -10084.7939],\n",
      "        [  2867.9839,   3699.0254,   2049.5671]])\n",
      "tensor([-173.7144,   34.8849])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7b97ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust weights & reset gradients\n",
    "with torch.no_grad():\n",
    "    w -= w.grad * 1e-5\n",
    "    b -= b.grad * 1e-5\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "48d2cc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1232, -0.1956, -1.0709],\n",
      "        [-0.4317,  1.7073,  0.2274]], requires_grad=True)\n",
      "tensor([-1.1792, -1.8988], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "22c08119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12114.2207, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate loss\n",
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e463b",
   "metadata": {},
   "source": [
    "## Train for multiple epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8a7a01c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    preds = model(inputs)\n",
    "    loss = mse(preds, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * 1e-5\n",
    "        b -= b.grad * 1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "12ccb99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(270.3501, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate loss\n",
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "48098a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 62.9919,  67.9583],\n",
       "        [ 77.8760,  93.0045],\n",
       "        [119.1718, 154.1657],\n",
       "        [ 55.2558,  24.5867],\n",
       "        [ 74.2117, 112.6805]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e609794b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d65fb68",
   "metadata": {},
   "source": [
    "## using pytorch built-ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "25e2ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1b1b60f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70], \n",
    "                   [74, 66, 43], \n",
    "                   [91, 87, 65], \n",
    "                   [88, 134, 59], \n",
    "                   [101, 44, 37], \n",
    "                   [68, 96, 71], \n",
    "                   [73, 66, 44], \n",
    "                   [92, 87, 64], \n",
    "                   [87, 135, 57], \n",
    "                   [103, 43, 36], \n",
    "                   [68, 97, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119],\n",
    "                    [57, 69], \n",
    "                    [80, 102], \n",
    "                    [118, 132], \n",
    "                    [21, 38], \n",
    "                    [104, 118], \n",
    "                    [57, 69], \n",
    "                    [82, 100], \n",
    "                    [118, 134], \n",
    "                    [20, 38], \n",
    "                    [102, 120]], \n",
    "                   dtype='float32')\n",
    "\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3f669db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 73.,  67.,  43.],\n",
       "        [ 91.,  88.,  64.],\n",
       "        [ 87., 134.,  58.],\n",
       "        [102.,  43.,  37.],\n",
       "        [ 69.,  96.,  70.],\n",
       "        [ 74.,  66.,  43.],\n",
       "        [ 91.,  87.,  65.],\n",
       "        [ 88., 134.,  59.],\n",
       "        [101.,  44.,  37.],\n",
       "        [ 68.,  96.,  71.],\n",
       "        [ 73.,  66.,  44.],\n",
       "        [ 92.,  87.,  64.],\n",
       "        [ 87., 135.,  57.],\n",
       "        [103.,  43.,  36.],\n",
       "        [ 68.,  97.,  70.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "62f81bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "696f06c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.]]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7e19e1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define data loader\n",
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4d2fbb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 91.,  88.,  64.],\n",
      "        [103.,  43.,  36.],\n",
      "        [101.,  44.,  37.],\n",
      "        [ 91.,  87.,  65.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 81., 101.],\n",
      "        [ 20.,  38.],\n",
      "        [ 21.,  38.],\n",
      "        [ 80., 102.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2b373c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3692, -0.0276,  0.0190],\n",
      "        [-0.3137, -0.5265, -0.0547]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.4914, 0.1170], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "model = nn.Linear(3, 2)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f6c29b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.3692, -0.0276,  0.0190],\n",
       "         [-0.3137, -0.5265, -0.0547]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.4914, 0.1170], requires_grad=True)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f49f2d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  26.4100,  -60.4079],\n",
       "        [  32.8748,  -78.2588],\n",
       "        [  30.0129, -100.8942],\n",
       "        [  37.6659,  -56.5410],\n",
       "        [  24.6454,  -75.8978],\n",
       "        [  26.8068,  -60.1951],\n",
       "        [  32.9215,  -77.7870],\n",
       "        [  30.4011, -101.2626],\n",
       "        [  37.2691,  -56.7538],\n",
       "        [  24.2952,  -75.6388],\n",
       "        [  26.4566,  -59.9361],\n",
       "        [  33.2716,  -78.0460],\n",
       "        [  29.9662, -101.3660],\n",
       "        [  38.0161,  -56.8000],\n",
       "        [  24.2485,  -76.1106]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e1a02bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nn.functional\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9836db1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16799.7285, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Define loss function\n",
    "loss_fn = F.mse_loss\n",
    "loss = loss_fn(model(inputs), targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b21ca6",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "010228be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3bb081",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ea8bb76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb, yb in train_dl:\n",
    "            \n",
    "            # 1. Generate predictions\n",
    "            pred = model(xb)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            \n",
    "            # 3. Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step()\n",
    "            \n",
    "            # 5. Reset the gradients to zero\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        # Print the progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5bca8dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 271.8492\n",
      "Epoch [20/100], Loss: 412.0470\n",
      "Epoch [30/100], Loss: 439.5253\n",
      "Epoch [40/100], Loss: 176.4637\n",
      "Epoch [50/100], Loss: 191.4943\n",
      "Epoch [60/100], Loss: 30.5090\n",
      "Epoch [70/100], Loss: 83.4896\n",
      "Epoch [80/100], Loss: 30.3621\n",
      "Epoch [90/100], Loss: 53.0347\n",
      "Epoch [100/100], Loss: 16.6919\n"
     ]
    }
   ],
   "source": [
    "fit(100, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3352e9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 58.7286,  71.6750],\n",
       "        [ 80.7548,  99.4447],\n",
       "        [118.6558, 133.3097],\n",
       "        [ 30.0977,  44.8099],\n",
       "        [ 94.2890, 112.4989],\n",
       "        [ 57.6704,  70.7392],\n",
       "        [ 80.2696,  99.2645],\n",
       "        [118.8212, 133.8070],\n",
       "        [ 31.1559,  45.7457],\n",
       "        [ 94.8621, 113.2545],\n",
       "        [ 58.2434,  71.4948],\n",
       "        [ 79.6966,  98.5089],\n",
       "        [119.1410, 133.4900],\n",
       "        [ 29.5246,  44.0543],\n",
       "        [ 95.3472, 113.4347]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e873d78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.],\n",
       "        [ 57.,  69.],\n",
       "        [ 80., 102.],\n",
       "        [118., 132.],\n",
       "        [ 21.,  38.],\n",
       "        [104., 118.],\n",
       "        [ 57.,  69.],\n",
       "        [ 82., 100.],\n",
       "        [118., 134.],\n",
       "        [ 20.,  38.],\n",
       "        [102., 120.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "edf5d86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.7286,  1.6750],\n",
       "        [-0.2452, -1.5553],\n",
       "        [-0.3442,  0.3097],\n",
       "        [ 8.0977,  7.8099],\n",
       "        [-8.7110, -6.5011],\n",
       "        [ 0.6704,  1.7392],\n",
       "        [ 0.2696, -2.7355],\n",
       "        [ 0.8212,  1.8070],\n",
       "        [10.1559,  7.7457],\n",
       "        [-9.1379, -4.7455],\n",
       "        [ 1.2434,  2.4948],\n",
       "        [-2.3034, -1.4911],\n",
       "        [ 1.1410, -0.5100],\n",
       "        [ 9.5246,  6.0543],\n",
       "        [-6.6528, -6.5653]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds - targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c23a7eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[55.2727, 68.8164]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[75, 63, 44.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fdacfb",
   "metadata": {},
   "source": [
    "# Working with Images & Logistic Regression in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b670eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9a2a415f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8880ee3dda68470b848dde7432e1ab0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc213a436ba45e5af8224fd3e580e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0057293c3af49da875de1a031591048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de45d27a2f644e4bc25656f860fe478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/home/bin/anaconda3/envs/good/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /tmp/pip-req-build-k42uejxm/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# Download training dataset\n",
    "dataset = MNIST(root='data/', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a7d215af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b19c1b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MNIST(root='data/', train=False)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7cf72b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28 at 0x7FF648567A90>, 5)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8c7dd8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "image, label = dataset[0]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "793a67f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6749cf16",
   "metadata": {},
   "source": [
    "PyTorch datasets allow us to specify one or more transformation functions that are applied to the images as they are loaded. The torchvision.transforms module contains many such predefined functions. We'll use the ToTensor transform to convert images into PyTorch tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2d0eb4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset (images and labels)\n",
    "dataset = MNIST(root='data/', \n",
    "                train=True,\n",
    "                transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "29bd198b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n"
     ]
    }
   ],
   "source": [
    "img_tensor, label = dataset[0]\n",
    "print(img_tensor.shape, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "107dbd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
      "        [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
      "        [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
      "        [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
      "        [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]])\n",
      "tensor(1.) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(img_tensor[0,10:15,10:15])\n",
    "print(torch.max(img_tensor), torch.min(img_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8c0e7513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff5d8489370>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the image by passing in the 28x28 matrix\n",
    "plt.imshow(img_tensor[0,10:15,10:15], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c95ebdbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [50000, 10000])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1552229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "18abace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_size = 28 * 28\n",
    "num_classes = 10\n",
    "\n",
    "# Logistic regression model\n",
    "model = nn.Linear(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ff367023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n"
     ]
    }
   ],
   "source": [
    "print(model.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f0177610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0093,  0.0228,  0.0141,  ...,  0.0109,  0.0265,  0.0267],\n",
       "        [-0.0005, -0.0066, -0.0300,  ...,  0.0090,  0.0182,  0.0148],\n",
       "        [-0.0146, -0.0049, -0.0334,  ...,  0.0299,  0.0073, -0.0206],\n",
       "        ...,\n",
       "        [-0.0060,  0.0007, -0.0086,  ..., -0.0107,  0.0346, -0.0158],\n",
       "        [ 0.0295, -0.0032,  0.0005,  ..., -0.0328, -0.0106, -0.0070],\n",
       "        [ 0.0200, -0.0270,  0.0183,  ...,  0.0106,  0.0109, -0.0356]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0028c1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0077,  0.0294,  0.0198,  0.0084,  0.0081,  0.0208,  0.0151,  0.0072,\n",
       "         0.0264, -0.0258], requires_grad=True)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8176471d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 5, 1, 5, 7, 6, 1, 8, 1, 9, 9, 1, 5, 9, 1, 5, 1, 1, 5, 3, 4, 8, 1, 4,\n",
      "        4, 3, 6, 8, 2, 6, 0, 1, 5, 7, 0, 0, 4, 9, 5, 1, 8, 1, 2, 0, 8, 7, 2, 1,\n",
      "        7, 5, 1, 8, 1, 3, 3, 4, 7, 6, 0, 0, 2, 9, 8, 6, 1, 5, 9, 7, 6, 4, 1, 0,\n",
      "        0, 1, 1, 7, 1, 1, 9, 4, 0, 7, 8, 0, 7, 2, 4, 7, 7, 1, 2, 1, 7, 1, 7, 6,\n",
      "        7, 3, 8, 8, 0, 0, 6, 7, 3, 1, 4, 3, 5, 0, 5, 6, 2, 3, 9, 5, 1, 9, 2, 6,\n",
      "        2, 1, 1, 7, 8, 6, 0, 1])\n",
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3584x28 and 784x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/39/glh0vc5x3qq23jgtd82s68200000gn/T/ipykernel_79375/3761977415.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/envs/good/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/envs/good/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/envs/good/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3584x28 and 784x10)"
     ]
    }
   ],
   "source": [
    "for images, labels, in train_loader:\n",
    "    print(labels)\n",
    "    print(images.shape)\n",
    "    outputs = model(images)\n",
    "    print(outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6eb6823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        return self.linear(xb)\n",
    "    \n",
    "model = MnistModel()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ac09422a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=10, bias=True)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8a5a3bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0030, -0.0327,  0.0098,  ...,  0.0128,  0.0157, -0.0149],\n",
       "         [-0.0331, -0.0265, -0.0348,  ...,  0.0254, -0.0030,  0.0093],\n",
       "         [ 0.0130,  0.0174, -0.0179,  ..., -0.0045, -0.0186, -0.0151],\n",
       "         ...,\n",
       "         [-0.0259,  0.0042, -0.0344,  ...,  0.0311, -0.0224, -0.0093],\n",
       "         [ 0.0111,  0.0038, -0.0060,  ..., -0.0297, -0.0204,  0.0026],\n",
       "         [-0.0330,  0.0124, -0.0279,  ..., -0.0008, -0.0310, -0.0256]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0356, -0.0035,  0.0318, -0.0132, -0.0203, -0.0014, -0.0023,  0.0157,\n",
       "         -0.0055, -0.0330], requires_grad=True)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.linear.weight.shape, model.linear.bias.shape)\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3bdad3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "outputs.shape :  torch.Size([128, 10])\n",
      "Sample outputs :\n",
      " tensor([[-0.1746,  0.0252, -0.2429,  0.0882,  0.1402,  0.1986,  0.0651,  0.0425,\n",
      "         -0.2185,  0.2959],\n",
      "        [ 0.2169,  0.0542, -0.4321, -0.0708,  0.2001,  0.0438, -0.0465,  0.1344,\n",
      "         -0.2962,  0.4839]])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "    outputs = model(images)\n",
    "    break\n",
    "    \n",
    "print('outputs.shape : ', outputs.shape)\n",
    "print('Sample outputs :\\n', outputs[:2].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3614287c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample probs: \n",
      " tensor([[0.0810, 0.0989, 0.0756, 0.1053, 0.1109, 0.1176, 0.1029, 0.1006, 0.0775,\n",
      "         0.1296],\n",
      "        [0.1171, 0.0995, 0.0612, 0.0878, 0.1151, 0.0985, 0.0900, 0.1078, 0.0701,\n",
      "         0.1529]])\n",
      "Sum:  0.9999999403953552\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Apply softmax for each output row\n",
    "probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "print('Sample probs: \\n', probs[:2].data)\n",
    "\n",
    "print('Sum: ', torch.sum(probs[0]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7d16bacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9, 9, 3, 7, 3, 1, 9, 3, 3, 1, 9, 9, 4, 6, 3, 9, 9, 5, 5, 9, 1, 9, 9, 6,\n",
      "        0, 4, 4, 3, 7, 6, 5, 3, 0, 9, 5, 4, 3, 1, 9, 9, 9, 0, 9, 0, 0, 9, 9, 4,\n",
      "        9, 9, 3, 9, 0, 0, 4, 7, 9, 3, 9, 4, 9, 7, 3, 6, 6, 4, 3, 3, 9, 0, 1, 0,\n",
      "        9, 0, 9, 9, 5, 4, 9, 7, 0, 9, 9, 3, 9, 9, 9, 0, 0, 4, 9, 3, 1, 4, 1, 7,\n",
      "        9, 9, 9, 3, 9, 6, 3, 1, 0, 0, 9, 1, 9, 3, 4, 7, 4, 7, 9, 4, 9, 9, 4, 0,\n",
      "        6, 0, 9, 0, 4, 0, 9, 5])\n",
      "tensor([0.1296, 0.1529, 0.1323, 0.1313, 0.1346, 0.1102, 0.1273, 0.1455, 0.1223,\n",
      "        0.1214, 0.1606, 0.1381, 0.1280, 0.1214, 0.1280, 0.1404, 0.1273, 0.1179,\n",
      "        0.1165, 0.1185, 0.1229, 0.1325, 0.1385, 0.1288, 0.1330, 0.1350, 0.1208,\n",
      "        0.1249, 0.1173, 0.1416, 0.1227, 0.1234, 0.1429, 0.1462, 0.1229, 0.1301,\n",
      "        0.1375, 0.1288, 0.1398, 0.1424, 0.1306, 0.1271, 0.1316, 0.1434, 0.1292,\n",
      "        0.1395, 0.1498, 0.1265, 0.1392, 0.1496, 0.1193, 0.1294, 0.1488, 0.1346,\n",
      "        0.1283, 0.1374, 0.1454, 0.1189, 0.1213, 0.1295, 0.1495, 0.1247, 0.1225,\n",
      "        0.1202, 0.1171, 0.1276, 0.1159, 0.1212, 0.1526, 0.1276, 0.1280, 0.1470,\n",
      "        0.1367, 0.1169, 0.1247, 0.1313, 0.1180, 0.1170, 0.1242, 0.1341, 0.1640,\n",
      "        0.1735, 0.1480, 0.1339, 0.1228, 0.1178, 0.1270, 0.1572, 0.1309, 0.1453,\n",
      "        0.1187, 0.1335, 0.1341, 0.1304, 0.1319, 0.1270, 0.1429, 0.1204, 0.1362,\n",
      "        0.1238, 0.1285, 0.1212, 0.1249, 0.1211, 0.1378, 0.1161, 0.1469, 0.1159,\n",
      "        0.1229, 0.1179, 0.1227, 0.1215, 0.1259, 0.1149, 0.1208, 0.1191, 0.1218,\n",
      "        0.1176, 0.1158, 0.1315, 0.1206, 0.1453, 0.1333, 0.1183, 0.1166, 0.1119,\n",
      "        0.1394, 0.1228], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_probs, preds = torch.max(probs, dim=1)\n",
    "\n",
    "print(preds)\n",
    "print(max_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d683d736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 4, 2, 5, 7, 3, 4, 8, 9, 8, 8, 3, 4, 4, 6, 9, 3, 0, 1, 5, 5, 8, 0,\n",
       "        6, 2, 0, 8, 1, 3, 2, 8, 8, 7, 2, 6, 8, 9, 8, 1, 1, 4, 1, 0, 4, 5, 1, 4,\n",
       "        9, 4, 0, 1, 6, 0, 0, 6, 5, 3, 2, 1, 4, 3, 8, 0, 7, 3, 7, 3, 5, 8, 7, 7,\n",
       "        9, 5, 5, 0, 3, 7, 2, 3, 0, 8, 3, 4, 9, 1, 0, 5, 8, 4, 0, 7, 5, 7, 5, 3,\n",
       "        4, 3, 2, 7, 8, 3, 0, 5, 8, 7, 9, 7, 1, 5, 5, 2, 0, 1, 7, 4, 0, 1, 3, 4,\n",
       "        0, 6, 1, 0, 7, 5, 4, 2])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b300e10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1094)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(preds == labels) / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ee84b1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "715da68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1094)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(outputs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a2c78",
   "metadata": {},
   "source": [
    "Accuracy is an excellent way for us (humans) to evaluate the model. However, we can't use it as a loss function for optimizing our model using gradient descent for the following reasons:\n",
    "\n",
    "    It's not a differentiable function. torch.max and == are both non-continuous and non-differentiable operations, so we can't use the accuracy for computing gradients w.r.t the weights and biases.\n",
    "\n",
    "    It doesn't take into account the actual probabilities predicted by the model, so it can't provide sufficient feedback for incremental improvements.\n",
    "\n",
    "For these reasons, accuracy is often used as an evaluation metric for classification, but not as a loss function. A commonly used loss function for classification problems is the cross-entropy, which has the following formula: D(y_hat, y) = - Sum(y_j ln(y_j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "22913561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3362, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = F.cross_entropy(outputs, labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "522b6d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    history = [] # for recording epoch-wise results\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "19992413",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a9921ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2 = [x*2 for x in l1]\n",
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d194c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4dd7c03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "    \n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "35eeec4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.3334498405456543, 'val_acc': 0.13221915066242218}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result0 = evaluate(model, val_loader)\n",
    "result0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654a9569",
   "metadata": {},
   "source": [
    "The initial accuracy is around 10%, which one might expect from a randomly initialized model (since it has a 1 in 10 chance of getting a label right by guessing randomly).\n",
    "\n",
    "We are now ready to train the model. Let's train for five epochs and look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fb4d1baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 1.9607, val_acc: 0.6128\n",
      "Epoch [1], val_loss: 1.6877, val_acc: 0.7236\n",
      "Epoch [2], val_loss: 1.4851, val_acc: 0.7590\n",
      "Epoch [3], val_loss: 1.3330, val_acc: 0.7768\n",
      "Epoch [4], val_loss: 1.2164, val_acc: 0.7897\n"
     ]
    }
   ],
   "source": [
    "history1 = fit(5, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f9d8016d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 1.2062, val_acc: 0.7915\n",
      "Epoch [1], val_loss: 1.1963, val_acc: 0.7927\n",
      "Epoch [2], val_loss: 1.1867, val_acc: 0.7938\n",
      "Epoch [3], val_loss: 1.1772, val_acc: 0.7949\n",
      "Epoch [4], val_loss: 1.1680, val_acc: 0.7965\n"
     ]
    }
   ],
   "source": [
    "history2 = fit(5, 0.0001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "effda9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.7507, val_acc: 0.8394\n",
      "Epoch [1], val_loss: 0.6242, val_acc: 0.8564\n",
      "Epoch [2], val_loss: 0.5607, val_acc: 0.8630\n",
      "Epoch [3], val_loss: 0.5212, val_acc: 0.8690\n",
      "Epoch [4], val_loss: 0.4940, val_acc: 0.8733\n"
     ]
    }
   ],
   "source": [
    "history3 = fit(5, 0.01, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c497fe6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.4739, val_acc: 0.8777\n",
      "Epoch [1], val_loss: 0.4580, val_acc: 0.8800\n",
      "Epoch [2], val_loss: 0.4454, val_acc: 0.8835\n",
      "Epoch [3], val_loss: 0.4347, val_acc: 0.8847\n",
      "Epoch [4], val_loss: 0.4261, val_acc: 0.8854\n"
     ]
    }
   ],
   "source": [
    "history4 = fit(5, 0.01, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2e1bcf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "histories =  [result0] + history1 + history2 + history3 + history4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a99c0268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.4252, val_acc: 0.8857\n",
      "Epoch [1], val_loss: 0.4244, val_acc: 0.8859\n",
      "Epoch [2], val_loss: 0.4236, val_acc: 0.8860\n",
      "Epoch [3], val_loss: 0.4229, val_acc: 0.8860\n",
      "Epoch [4], val_loss: 0.4221, val_acc: 0.8860\n",
      "Epoch [5], val_loss: 0.4213, val_acc: 0.8861\n",
      "Epoch [6], val_loss: 0.4206, val_acc: 0.8865\n",
      "Epoch [7], val_loss: 0.4198, val_acc: 0.8866\n",
      "Epoch [8], val_loss: 0.4191, val_acc: 0.8867\n",
      "Epoch [9], val_loss: 0.4184, val_acc: 0.8866\n"
     ]
    }
   ],
   "source": [
    "histories.extend(fit(10, 0.001, model, train_loader, val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4eddfa27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.3764, val_acc: 0.8943\n",
      "Epoch [1], val_loss: 0.3559, val_acc: 0.8994\n",
      "Epoch [2], val_loss: 0.3450, val_acc: 0.9026\n"
     ]
    }
   ],
   "source": [
    "histories.extend(fit(3, 0.1, model, train_loader, val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "11f8c791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.3366, val_acc: 0.9057\n",
      "Epoch [1], val_loss: 0.3317, val_acc: 0.9063\n",
      "Epoch [2], val_loss: 0.3264, val_acc: 0.9086\n",
      "Epoch [3], val_loss: 0.3238, val_acc: 0.9081\n",
      "Epoch [4], val_loss: 0.3199, val_acc: 0.9104\n",
      "Epoch [5], val_loss: 0.3172, val_acc: 0.9110\n",
      "Epoch [6], val_loss: 0.3153, val_acc: 0.9116\n",
      "Epoch [7], val_loss: 0.3144, val_acc: 0.9115\n",
      "Epoch [8], val_loss: 0.3125, val_acc: 0.9131\n",
      "Epoch [9], val_loss: 0.3131, val_acc: 0.9127\n"
     ]
    }
   ],
   "source": [
    "histories.extend(fit(10, 0.1, model, train_loader, val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "fc1ab586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracies = [result['val_acc'] for result in histories]\n",
    "plt.plot(accuracies, '-x')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd46905",
   "metadata": {},
   "source": [
    "It's quite clear from the above picture that the model probably won't cross the accuracy threshold of 90% even after training for a very long time. One possible reason for this is that the learning rate might be too high. The model's parameters may be \"bouncing\" around the optimal set of parameters for the lowest loss. You can try reducing the learning rate and training for a few more epochs to see if it helps.\n",
    "\n",
    "The more likely reason that the model just isn't powerful enough. If you remember our initial hypothesis, we have assumed that the output (in this case the class probabilities) is a linear function of the input (pixel intensities), obtained by perfoming a matrix multiplication with the weights matrix and adding the bias. This is a fairly weak assumption, as there may not actually exist a linear relationship between the pixel intensities in an image and the digit it represents. While it works reasonably well for a simple dataset like MNIST (getting us to 85% accuracy), we need more sophisticated models that can capture non-linear relationships between image pixels and labels for complex tasks like recognizing everyday objects, animals etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2a8ac3",
   "metadata": {},
   "source": [
    "## Testing with individual images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b11ad214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test dataset\n",
    "test_dataset = MNIST(root='data/', \n",
    "                     train=False,\n",
    "                     transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e82f6da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([1, 28, 28])\n",
      "Label: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[0]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Shape:', img.shape)\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7b96262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    xb = img.unsqueeze(0)\n",
    "    yb = model(xb)\n",
    "    _, preds = torch.max(yb, dim=1)\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0d73aceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7 , Predicted: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[0]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b64f45f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0 , Predicted: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = test_dataset[10]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b7a7ef93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.27865976095199585, 'val_acc': 0.921093761920929}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "result = evaluate(model, test_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9838785b",
   "metadata": {},
   "source": [
    "# Saving and loading the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "61ff601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mnist-logistic.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2bb84995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[-0.0255,  0.0236,  0.0340,  ...,  0.0076,  0.0081, -0.0187],\n",
       "                      [-0.0193,  0.0146,  0.0147,  ..., -0.0296, -0.0280,  0.0240],\n",
       "                      [-0.0207,  0.0296, -0.0122,  ...,  0.0239, -0.0155, -0.0025],\n",
       "                      ...,\n",
       "                      [-0.0044,  0.0236, -0.0137,  ..., -0.0185, -0.0196, -0.0320],\n",
       "                      [ 0.0046,  0.0224,  0.0195,  ...,  0.0344,  0.0273, -0.0241],\n",
       "                      [-0.0131,  0.0199, -0.0325,  ..., -0.0265,  0.0268, -0.0113]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.3810,  0.3320,  0.0856, -0.2396,  0.0251,  1.2420, -0.1183,  0.6515,\n",
       "                      -1.4571, -0.2522]))])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4f4b1c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear.weight',\n",
       "              tensor([[-0.0255,  0.0236,  0.0340,  ...,  0.0076,  0.0081, -0.0187],\n",
       "                      [-0.0193,  0.0146,  0.0147,  ..., -0.0296, -0.0280,  0.0240],\n",
       "                      [-0.0207,  0.0296, -0.0122,  ...,  0.0239, -0.0155, -0.0025],\n",
       "                      ...,\n",
       "                      [-0.0044,  0.0236, -0.0137,  ..., -0.0185, -0.0196, -0.0320],\n",
       "                      [ 0.0046,  0.0224,  0.0195,  ...,  0.0344,  0.0273, -0.0241],\n",
       "                      [-0.0131,  0.0199, -0.0325,  ..., -0.0265,  0.0268, -0.0113]])),\n",
       "             ('linear.bias',\n",
       "              tensor([-0.3810,  0.3320,  0.0856, -0.2396,  0.0251,  1.2420, -0.1183,  0.6515,\n",
       "                      -1.4571, -0.2522]))])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = MnistModel()\n",
    "model2.load_state_dict(torch.load('mnist-logistic.pth'))\n",
    "model2.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9e6445e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.27865976095199585, 'val_acc': 0.921093761920929}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "result = evaluate(model2, test_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20bdb38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e328c3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60828997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b59aa08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff6ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e3eefa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746ab197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e2d179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d6df2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d4e5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240683bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15193cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc8caf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b57c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e84540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3880a300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c3c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a604395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533218fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
