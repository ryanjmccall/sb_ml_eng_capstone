{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3a812a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.24.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn \n",
    "\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9008ad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ce5b809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba205336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 784)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 1000\n",
    "images = X_train[:sample].reshape(sample, 28 * 28) / 255\n",
    "images.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ace3e500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "labels = y_train[:sample].reshape(-1, 1)\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(labels)\n",
    "enc.categories_\n",
    "\n",
    "\n",
    "labels = enc.transform(labels).toarray()\n",
    "labels[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95197029",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = X_test.reshape(len(X_test), 28 * 28) / 255\n",
    "test_labels = enc.transform(y_test.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83aa38b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "relu = lambda x: (x >= 0) * x\n",
    "relu2deriv = lambda x: x >= 0\n",
    "alpha, iterations, hidden_size, pixels_per_image, num_labels = \\\n",
    "                                              (0.005, 350, 40, 784, 10)\n",
    "weights_0_1 = 0.2 * np.random.random((pixels_per_image,hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2 * np.random.random((hidden_size,num_labels)) - 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75e7fed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I:349 Error:0.088 Correct:0.999"
     ]
    }
   ],
   "source": [
    "for j in range(iterations):\n",
    "    error, correct_cnt = (0.0, 0)\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        layer_0 = images[i:i+1]\n",
    "        layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "        layer_2 = np.dot(layer_1,weights_1_2)\n",
    "        error += np.sum((labels[i:i+1] - layer_2) ** 2)\n",
    "        correct_cnt += int(np.argmax(layer_2) == \\\n",
    "                                        np.argmax(labels[i:i+1]))\n",
    "        layer_2_delta = (labels[i:i+1] - layer_2)\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T)\\\n",
    "                                    * relu2deriv(layer_1)\n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "\n",
    "    sys.stdout.write(\"\\r\"+ \\\n",
    "                     \"Epoch: \"+str(j)+ \\\n",
    "                     \" Error: \" + str(error/float(len(images)))[0:5] +\\\n",
    "                     \" Correct: \" + str(correct_cnt/float(len(images))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9f059c",
   "metadata": {},
   "source": [
    "## Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b7e0499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n",
      " Test-Err:0.738 Test-Acc:0.686\n"
     ]
    }
   ],
   "source": [
    "for j in range(iterations):\n",
    "    error, correct_cnt = (0.0, 0)\n",
    "    if(j % 10 == 0 or j == iterations-1):\n",
    "      error, correct_cnt = (0.0, 0)\n",
    "\n",
    "      for i in range(len(test_images)):\n",
    "\n",
    "            layer_0 = test_images[i:i+1]\n",
    "            layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "            layer_2 = np.dot(layer_1,weights_1_2)\n",
    "\n",
    "            error += np.sum((test_labels[i:i+1] - layer_2) ** 2)\n",
    "            correct_cnt += int(np.argmax(layer_2) == \\\n",
    "                                            np.argmax(test_labels[i:i+1]))\n",
    "      sys.stdout.write(\" Test-Err:\" + str(error/float(len(test_images)))[0:5] +\\\n",
    "                 \" Test-Acc:\" + str(correct_cnt/float(len(test_images))))\n",
    "      print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd78b92d",
   "metadata": {},
   "source": [
    "Dropout 50% of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f88d66c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_mask = np.random.randint(2,size=layer_1.shape)\n",
    "dropout_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c30eb98",
   "metadata": {},
   "source": [
    "You multiply layer_1 by 2. Why do you do this? Remember that layer_2 will perform a weighted sum of layer_1. Even though it’s weighted, it’s still a sum over the values of layer_1. If you turn off half the nodes in layer_1, that sum will be cut in half. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a1ae8446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Test-Err:0.632 Test-Acc:0.6388571428571429 Train-Err:0.896 Train-Acc:0.393\n",
      "I:10 Test-Err:0.433 Test-Acc:0.786 Train-Err:0.479 Train-Acc:0.746\n",
      "I:20 Test-Err:0.400 Test-Acc:0.8215 Train-Err:0.451 Train-Acc:0.783\n",
      "I:30 Test-Err:0.395 Test-Acc:0.8272142857142857 Train-Err:0.424 Train-Acc:0.81\n",
      "I:40 Test-Err:0.385 Test-Acc:0.8395714285714285 Train-Err:0.419 Train-Acc:0.826\n",
      "I:50 Test-Err:0.394 Test-Acc:0.8359285714285715 Train-Err:0.433 Train-Acc:0.81\n",
      "I:60 Test-Err:0.400 Test-Acc:0.8310714285714286 Train-Err:0.427 Train-Acc:0.803\n",
      "I:70 Test-Err:0.383 Test-Acc:0.8357857142857142 Train-Err:0.410 Train-Acc:0.818\n",
      "I:80 Test-Err:0.387 Test-Acc:0.8302142857142857 Train-Err:0.393 Train-Acc:0.845\n",
      "I:90 Test-Err:0.378 Test-Acc:0.8209285714285715 Train-Err:0.390 Train-Acc:0.851\n",
      "I:100 Test-Err:0.376 Test-Acc:0.8196428571428571 Train-Err:0.389 Train-Acc:0.838\n",
      "I:110 Test-Err:0.370 Test-Acc:0.8242857142857143 Train-Err:0.379 Train-Acc:0.862\n",
      "I:120 Test-Err:0.386 Test-Acc:0.8200714285714286 Train-Err:0.381 Train-Acc:0.848\n",
      "I:130 Test-Err:0.369 Test-Acc:0.8247142857142857 Train-Err:0.373 Train-Acc:0.851\n",
      "I:140 Test-Err:0.375 Test-Acc:0.8218571428571428 Train-Err:0.366 Train-Acc:0.863\n",
      "I:150 Test-Err:0.381 Test-Acc:0.8309285714285715 Train-Err:0.357 Train-Acc:0.88\n",
      "I:160 Test-Err:0.378 Test-Acc:0.8260714285714286 Train-Err:0.359 Train-Acc:0.865\n",
      "I:170 Test-Err:0.377 Test-Acc:0.8223571428571429 Train-Err:0.366 Train-Acc:0.86\n",
      "I:180 Test-Err:0.387 Test-Acc:0.8224285714285714 Train-Err:0.355 Train-Acc:0.875\n",
      "I:190 Test-Err:0.380 Test-Acc:0.82 Train-Err:0.345 Train-Acc:0.877\n",
      "I:200 Test-Err:0.386 Test-Acc:0.8206428571428571 Train-Err:0.343 Train-Acc:0.883\n",
      "I:210 Test-Err:0.383 Test-Acc:0.8142857142857143 Train-Err:0.351 Train-Acc:0.869\n",
      "I:220 Test-Err:0.382 Test-Acc:0.8175714285714286 Train-Err:0.353 Train-Acc:0.873\n",
      "I:230 Test-Err:0.384 Test-Acc:0.8185 Train-Err:0.346 Train-Acc:0.88\n",
      "I:240 Test-Err:0.381 Test-Acc:0.813 Train-Err:0.345 Train-Acc:0.877\n",
      "I:250 Test-Err:0.389 Test-Acc:0.8169285714285714 Train-Err:0.350 Train-Acc:0.879\n",
      "I:260 Test-Err:0.375 Test-Acc:0.8202857142857143 Train-Err:0.336 Train-Acc:0.892\n",
      "I:270 Test-Err:0.380 Test-Acc:0.8240714285714286 Train-Err:0.331 Train-Acc:0.886\n",
      "I:280 Test-Err:0.382 Test-Acc:0.8237857142857142 Train-Err:0.335 Train-Acc:0.886\n",
      "I:290 Test-Err:0.378 Test-Acc:0.8254285714285714 Train-Err:0.336 Train-Acc:0.897"
     ]
    }
   ],
   "source": [
    "import numpy, sys\n",
    "np.random.seed(1)\n",
    "def relu(x):\n",
    "   return (x >= 0) * x\n",
    "\n",
    "def relu2deriv(output):\n",
    "   return output >= 0\n",
    "\n",
    "alpha, iterations, hidden_size = (0.005, 300, 100)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((pixels_per_image,hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1\n",
    "\n",
    "for j in range(iterations):\n",
    "   error, correct_cnt = (0.0,0)\n",
    "   for i in range(len(images)):\n",
    "      layer_0 = images[i:i+1]\n",
    "      layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "      dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "      layer_1 *= dropout_mask * 2\n",
    "      layer_2 = np.dot(layer_1,weights_1_2)\n",
    "\n",
    "      error += np.sum((labels[i:i+1] - layer_2) ** 2)\n",
    "      correct_cnt += int(np.argmax(layer_2) == \\\n",
    "                                      np.argmax(labels[i:i+1]))\n",
    "      layer_2_delta = (labels[i:i+1] - layer_2)\n",
    "      layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "      layer_1_delta *= dropout_mask\n",
    "\n",
    "      weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "      weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "\n",
    "   if(j%10 == 0):\n",
    "      test_error = 0.0\n",
    "      test_correct_cnt = 0\n",
    "\n",
    "      for i in range(len(test_images)):\n",
    "           layer_0 = test_images[i:i+1]\n",
    "           layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "           layer_2 = np.dot(layer_1, weights_1_2)\n",
    "\n",
    "           test_error += np.sum((test_labels[i:i+1] - layer_2) ** 2)\n",
    "           test_correct_cnt += int(np.argmax(layer_2) == \\\n",
    "                                     np.argmax(test_labels[i:i+1]))\n",
    "\n",
    "      sys.stdout.write(\"\\n\" + \\\n",
    "           \"I:\" + str(j) + \\\n",
    "           \" Test-Err:\" + str(test_error/ float(len(test_images)))[0:5] +\\\n",
    "           \" Test-Acc:\" + str(test_correct_cnt/ float(len(test_images)))+\\\n",
    "           \" Train-Err:\" + str(error/ float(len(images)))[0:5] +\\\n",
    "           \" Train-Acc:\" + str(correct_cnt/ float(len(images))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41aeb5d",
   "metadata": {},
   "source": [
    "## Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a78d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "def relu(x):\n",
    "    return (x >= 0) * x\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return output >= 0\n",
    "\n",
    "batch_size = 100\n",
    "alpha, iterations = (0.001, 300)\n",
    "pixels_per_image, num_labels, hidden_size = (784, 10, 100)\n",
    "\n",
    "weights_0_1 = 0.2*np.random.random((pixels_per_image,hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2*np.random.random((hidden_size,num_labels)) - 0.1\n",
    "\n",
    "for j in range(iterations):\n",
    "    error, correct_cnt = (0.0, 0)\n",
    "    for i in range(int(len(images) / batch_size)):\n",
    "        batch_start, batch_end = ((i * batch_size),((i+1)*batch_size))\n",
    "\n",
    "        layer_0 = images[batch_start:batch_end]\n",
    "        layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "        dropout_mask = np.random.randint(2,size=layer_1.shape)\n",
    "        layer_1 *= dropout_mask * 2\n",
    "        layer_2 = np.dot(layer_1,weights_1_2)\n",
    "\n",
    "        error += np.sum((labels[batch_start:batch_end] - layer_2) ** 2)\n",
    "        for k in range(batch_size):\n",
    "            correct_cnt += int(np.argmax(layer_2[k:k+1]) == \\\n",
    "                    np.argmax(labels[batch_start+k:batch_start+k+1]))\n",
    "\n",
    "            layer_2_delta = (labels[batch_start:batch_end]-layer_2) \\\n",
    "                                                            /batch_size\n",
    "            layer_1_delta = layer_2_delta.dot(weights_1_2.T)* \\\n",
    "                                                     relu2deriv(layer_1)\n",
    "            layer_1_delta *= dropout_mask\n",
    "\n",
    "            weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "            weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "\n",
    "    if(j%10 == 0):\n",
    "        test_error = 0.0\n",
    "        test_correct_cnt = 0\n",
    "\n",
    "        for i in range(len(test_images)):\n",
    "            layer_0 = test_images[i:i+1]\n",
    "            layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "            layer_2 = np.dot(layer_1, weights_1_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff0eeb2",
   "metadata": {},
   "source": [
    "The first thing you’ll notice when running this code is that it runs much faster. This is because each np.dot function is now performing 100 vector dot products at a time. CPU architectures are much faster at performing dot products batched this way. \n",
    "\n",
    "There’s more going on here, however. Notice that alpha is 20 times larger than before. You can increase it for a fascinating reason. Imagine you were trying to find a city using a very wobbly compass. If you looked down, got a heading, and then ran 2 miles, you’d likely be way off course. But if you looked down, took 100 headings, and then averaged them, running 2 miles would probably take you in the general right direction.\n",
    "\n",
    "Because the example takes an average of a noisy signal (the average weight change over 100 training examples), it can take bigger steps. You’ll generally see batching ranging from size 8 to as high as 256. Generally, researchers pick numbers randomly until they find a batch_size/alpha pair that seems to work well.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
