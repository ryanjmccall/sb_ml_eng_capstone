{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b2752a-b25a-4284-83f6-4e4359f24a53",
   "metadata": {},
   "source": [
    "# Machine Learning Engineering Career Track \n",
    "## Step 6: Scale Your Prototype\n",
    "\n",
    "In this step, your goal is to ensure that your ML/DL approach, which has proved to be viable, can work with large volumes of data. Please work with your mentor to determine what that means for your problem.\n",
    "Using scikit-learn, SparkML, Keras, TensorFlow, PyTorch or some of the other technologies you have learned, implement your prototype at scale.\n",
    "In case your earlier prototype was working with a subset, ensure that this scaled-up prototype can handle your complete dataset.\n",
    "Think about what your capstone problem would look like in the real world:\n",
    "- How much data would you need to handle?\n",
    "- Can you scale your prototype to handle that volume of data using the approach and tools you have selected?\n",
    "\n",
    "Implement the scaled version of your prototype and clearly document the trade-offs and implementation decisions you have to make to be able to scale your algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03560c8-66db-4ccc-8850-5b40707f83a4",
   "metadata": {},
   "source": [
    "## Outline\n",
    "- in the previous prototype notebook I only ran hyperparameter search on 10% of the data \n",
    "- here I will run all the data and perform a longer-running search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cab2372-bcb2-4dd7-b6eb-8945fe70d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, floor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.pipeline import Pipeline \n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import (silhouette_score, f1_score, plot_confusion_matrix, \n",
    "                             plot_precision_recall_curve, precision_score, recall_score)\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.callbacks import TimerCallback, DeadlineStopper\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from tqdm.notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e574524c-0dcc-4867-b29d-f0bf3d6b34a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limit  110250\n",
      "sr  22050.0\n",
      "X  (13700, 160)  y  (13700,)\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = 'data/features/df.pkl'\n",
    "df = pd.read_pickle(checkpoint_path)\n",
    "\n",
    "extract_limit = df.extract_limit.values[0]  # previously found good audio limit\n",
    "sr = df.sr.values[0] # sample rate\n",
    "\n",
    "# NB cannot use select_dtypes since features are stored under *int* index *names*\n",
    "X = df[[c for c in df.columns.values if isinstance(c, int)]] #  + ['audio', 'sr']\n",
    "y = df.negativity\n",
    "\n",
    "print('limit ', extract_limit)\n",
    "print('sr ', sr)\n",
    "print('X ', X.shape, ' y ', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1928578-7008-4c69-87ee-cdf4f226bb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10960, 160), (2740, 160), (10960,), (2740,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, stratify=y, random_state=0)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f097b20-3e14-45ed-bb23-53906d068965",
   "metadata": {},
   "source": [
    "Bring in model and pipeline with hyperparameters established in previous notebook..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "696eafb5-85e9-456f-a32a-cac7d7584b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    return lgb.LGBMClassifier( \n",
    "    \tobjective='binary', \n",
    "    \tmetric='binary_logloss', \n",
    "    \tclass_weight='balanced', \n",
    "    \trandom_state=0, \n",
    "    \tn_jobs=-1,\n",
    "        num_leaves=95,\n",
    "        n_estimators=320,\n",
    "        max_depth=14,\n",
    "        boosting_type='dart',\n",
    "        learning_rate=0.5,\n",
    "        min_split_gain=0,\n",
    "        min_child_weight=1e-05,\n",
    "        min_child_samples=30,\n",
    "        subsample=0.97175,\n",
    "        colsample_bytree=0.95,\n",
    "        subsample_freq=2,\n",
    "        reg_alpha=0,\n",
    "        reg_lambda=0\n",
    "    )\n",
    "\n",
    "\n",
    "def get_pipe():\n",
    "    return Pipeline([\n",
    "        ('oversampling', ADASYN(random_state=0, n_jobs=-1)), \n",
    "        ('standardization', QuantileTransformer(output_distribution='normal', random_state=0)),\n",
    "        ('decomposition', PCA(n_components=50, random_state=0)),\n",
    "        ('model', get_model()),\n",
    "    ])\n",
    "\n",
    "\n",
    "def bayes_search(X, y, pipe, spaces: dict, n_iter:int, refit=False):\n",
    "    \"\"\"Wrapper to simplify hyperparameter experimentation\"\"\"\n",
    "    budget_minutes = 120 # time-based stopping criterion\n",
    "    cbs = [TimerCallback(), DeadlineStopper(60 * budget_minutes)]\n",
    "    opt = BayesSearchCV(\n",
    "        estimator=pipe,\n",
    "        search_spaces=spaces,\n",
    "        n_iter=n_iter,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        n_points=1,  # number of parameter settings to sample in parallel\n",
    "        refit=refit,  # after opt, refits on entire dataset, so predictions can be made\n",
    "        verbose=0,\n",
    "        cv=3\n",
    "    )\n",
    "    res = opt.fit(X, y, callback=cbs)\n",
    "    iter_times = cbs[0].iter_time\n",
    "    print(f'elapsed time: {sum(iter_times)/60:.2f}m ave-iter={np.mean(iter_times):.2f}s')\n",
    "    return res\n",
    "\n",
    "\n",
    "def plot_spaces(search): \n",
    "    score = 'mean_test_score'\n",
    "    for space in search.search_spaces:        \n",
    "        plt.scatter(search.cv_results_['param_' + space], search.cv_results_[score])  \n",
    "        plt.xlabel(space.replace('model__', ''))\n",
    "        plt.ylabel(score)\n",
    "        plt.axvline(x=search.best_params_[space], color='orange')\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5107e62a-2167-4d07-ac2b-e7a963088e0c",
   "metadata": {},
   "source": [
    "Run a quick check training on 80% of the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9000c571-2525-4751-a63b-9bcf327b2c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9989963083169126, 0.6058228729479428)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = get_pipe()\n",
    "pipe.fit(X_train, y_train)\n",
    "train_score = f1_score(y_train, pipe.predict(X_train), average='weighted')\n",
    "test_score = f1_score(y_test, pipe.predict(X_test), average='weighted')\n",
    "train_score, test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18759e39-124c-43fb-a336-2b61a71370fb",
   "metadata": {},
   "source": [
    "Test perf on 80% of the data is about 0.003 worse (f1) but I had accepted this small amount of loss-of-perf for increased speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b90dc20-d25f-4b94-87c7-0c8af8f29232",
   "metadata": {},
   "source": [
    "Now I revisit a hyperparameter search using 80% of the data and setting the search spaces to be centered around the values found using 20% of the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfbaf52-b7f7-46e5-8f32-b5ff1c613feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spaces = {\n",
    "    # Power\n",
    "    'model__num_leaves': Integer(90, 100),                                       # def=31\n",
    "    'model__n_estimators': Integer(low=300, high=340),                           # def=100    \n",
    "    'model__max_depth': Integer(10, 18),                                         # def=-1 no limit\n",
    "\n",
    "    # Learning\n",
    "    'model__learning_rate': Real(low=0.1, high=0.9, prior='log-uniform'),           # def=0.1\n",
    "    'model__min_child_weight': Real(low=0.000001, high=0.001, prior='log-uniform'), # def=1e-3\n",
    "    'model__min_child_samples': Integer(20, 40),                                    # def=20\n",
    "\n",
    "    # Bagging\n",
    "    'model__subsample': Real(0.96, 0.98),                                          # def=1.0\n",
    "    'model__colsample_bytree': Real(0.92, 0.98),                                   # def=1.0\n",
    "}\n",
    "\n",
    "pipe = get_pipe()\n",
    "\n",
    "ave_iter = 15.37\n",
    "minutes = 30\n",
    "n_iter = int(minutes * 60 / ave_iter) \n",
    "\n",
    "search = bayes_search(X_train, y_train, pipe, spaces, n_iter, refit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d0c23-7a15-46bf-a0a2-7657f561e8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spaces(search)\n",
    "for k, v in search.best_params_.items():\n",
    "    print(k.replace('model__', ''), v)\n",
    "    \n",
    "print('\\nbest cv score=', search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf96809-859e-46d2-a6c8-65a51c3bb127",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
